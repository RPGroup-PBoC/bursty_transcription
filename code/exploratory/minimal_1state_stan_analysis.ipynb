{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "\n",
    "import re #regex\n",
    "\n",
    "import cmdstanpy\n",
    "import arviz as az\n",
    "\n",
    "import bebi103\n",
    "import bokeh_catplot\n",
    "\n",
    "import bokeh.io\n",
    "import bokeh.plotting\n",
    "# import bokeh.models.mappers\n",
    "import bokeh.palettes\n",
    "\n",
    "import holoviews as hv\n",
    "import holoviews.operation.datashader\n",
    "hv.extension('bokeh')\n",
    "bebi103.hv.set_defaults()\n",
    "\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Munging\n",
    "Load data from Brewster, pre-tidied by Manuel, and drop the spurious column that was the index in csv.\n",
    "See `code/exploratory/fish_munging.ipynb` for details. TL;DR: don't use the regulated csv, the one below has all the FISH data. mRNA_cell is the data we want, not spots_totals (some of the repressed strains have higher spots_totals than UV5, so that's clearly not the readout we want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fish = pd.read_csv(\"../../data/jones_brewster_2014.csv\")\n",
    "del df_fish['Unnamed: 0']\n",
    "df_fish.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get the energies from the supplement of Brewster/Jones 2012 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energies = pd.read_csv(\"../../data/brewster_jones_2012.csv\")\n",
    "df_energies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the promoters in the 2012 dataset are in the 2014 fish dataset (verified in `code/exploratory/fish_munging.ipynb`). These are the only constitutive promoters I'm interested in (this only excludes a couple, and they are useless without more metadata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into regulated & constitutive data\n",
    "Some of these datasets are not of interest right now so let's split it into multiple dataframes for easier downstream handling. The regulated datasets start with O1, O2, or O3. Everything else doesn't. From that everything else, grab the ones that we have energies for, and set aside the rest. Use regex to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_expt_labels = df_fish['experiment'].unique()\n",
    "raw_expt_labels.sort()\n",
    "\n",
    "# put all strings that start w/ 'O' in one list\n",
    "regulated_labels = [label for label in raw_expt_labels if re.match('^O', label)]\n",
    "# from that, split out those we have energies for\n",
    "constitutive_labels = [label for label in raw_expt_labels if label in tuple(df_energies.Name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without more metadata, I don't really know what to do with the leftover labels data, e.g., what good does the aTc concentration do me if I don't know what promoter it was for?\n",
    "\n",
    "Now that we've got labels we want, let's slice dataframes accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = df_fish[df_fish['experiment'].isin(regulated_labels)]\n",
    "df_unreg = df_fish[df_fish['experiment'].isin(constitutive_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also separate UV5 for testing convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UV5 = df_unreg[df_unreg[\"experiment\"] == \"UV5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing constitutive UV5 with Stan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stan model borrows from JB's tutorial 7a, 2018. Stan parametrizes the negative binomial with $\\alpha$ and $\\beta$, where $\\alpha$ is the burst frequency (dimensionless, nondimensionalized by mRNA lifetime) and $\\beta = 1/b$ where $b$ is the mean burst size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior predictive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_prior_predictive = cmdstanpy.CmdStanModel(\n",
    "    stan_file=\"stan/constitutive_prior_predictive_v01.stan\"\n",
    ")\n",
    "# print(sm_prior_predictive.code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_prior_pred = dict(\n",
    "#     N=len(df_UV5), \n",
    "#     log_alpha_loc=0.0, \n",
    "#     log_alpha_scale=2.0,\n",
    "#     log_b_loc=0.5,\n",
    "#     log_b_scale=1.0\n",
    "# )\n",
    "data_prior_pred = dict(\n",
    "    N=len(df_UV5), \n",
    "    log_alpha_loc=0.0, \n",
    "    log_alpha_scale=0.5,\n",
    "    log_b_loc=0.5,\n",
    "    log_b_scale=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_pred_samples = sm_prior_predictive.sample(\n",
    "    data=data_prior_pred,\n",
    "    fixed_param=True,\n",
    "    sampling_iters=1000,\n",
    "    output_dir=\"./stan/stan_samples\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ArviZ InferenceData\n",
    "prior_pred_samples = az.from_cmdstanpy(\n",
    "    posterior=prior_pred_samples,\n",
    "    prior=prior_pred_samples,\n",
    "    prior_predictive=['mRNA_counts']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = bebi103.viz.predictive_ecdf(\n",
    "    prior_pred_samples.prior_predictive['mRNA_counts'],\n",
    "    frame_height=250,\n",
    "    frame_width=350,\n",
    "    discrete=True,\n",
    "    percentiles=(95, 90, 75, 50),\n",
    "    x_axis_label='mRNA counts',\n",
    "    x_axis_type='log'\n",
    ")\n",
    "bokeh.io.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation-based calibration\n",
    "\n",
    "Next up: simulation-based calibration (SBC). Quoting JB, this checks \"that the sampler can effectively sample the entire space of parameters covered by the prior.\" We'll go ahead and set up the data for the posterior, even though we won't be sampling the posterior right now. Then we can set up the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_UV5 = copy.deepcopy(data_prior_pred)\n",
    "data_UV5[\"N\"] = len(df_UV5)\n",
    "data_UV5[\"mRNA_counts\"] = df_UV5[\"mRNA_cell\"].values.astype(int)\n",
    "data_UV5[\"ppc\"] = 0\n",
    "\n",
    "sm = cmdstanpy.CmdStanModel(stan_file=\"stan/constitutive_v01.stan\")\n",
    "# print(sm.code())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sbc_output = pd.read_csv(\"stan/sbc_minimal_1state_stan_analysis.csv\")\n",
    "except:\n",
    "    sbc_output = bebi103.stan.sbc(\n",
    "        prior_predictive_model=sm_prior_predictive,\n",
    "        posterior_model=sm,\n",
    "        prior_predictive_model_data=data_prior_pred,\n",
    "        posterior_model_data=data_UV5,\n",
    "        measured_data=[\"mRNA_counts\"],\n",
    "        parameters=[\"alpha\", \"b\"],\n",
    "        sampling_kwargs={\"thin\": 10},\n",
    "        cores=7,\n",
    "        N=400,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "    sbc_output.to_csv(\"stan/sbc_minimal_1state_stan_analysis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ECDFs of the rank statistics, which should be ~ uniform. Color according to warning code, which will also let us know if there were any issues with divergences, R-hat, EBFMI, effective number of steps, or tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [\n",
    "    bokeh_catplot.ecdf(\n",
    "        data=sbc_output.loc[sbc_output[\"parameter\"] == param, :],\n",
    "        val=\"rank_statistic\",\n",
    "        cats=\"warning_code\",\n",
    "        kind=\"colored\",\n",
    "        frame_width=400,\n",
    "        frame_height=150,\n",
    "        title=param,\n",
    "        conf_int=True,\n",
    "    )\n",
    "    for param in sbc_output[\"parameter\"].unique()\n",
    "]\n",
    "\n",
    "bokeh.io.show(bokeh.layouts.gridplot(plots, ncols=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very uniform. Warning code 2 means R-hat failure. Let's look at the R-hat values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.show(\n",
    "    bokeh_catplot.ecdf(data=sbc_output, val=\"Rhat\", cats=\"parameter\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terrible - about 15% are above the brightline 1.01, but not way over. JB suggests in this case that more samples from the posterior might be wise.\n",
    "\n",
    "JB also has a nice helper function to plot the difference from uniform of rank ECDFs, with a confidence interval for the uniform distribution to guide the eye. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.show(bebi103.viz.sbc_rank_ecdf(sbc_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's check shrinkage and z-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Points(\n",
    "    data=sbc_output,\n",
    "    kdims=['shrinkage', 'z_score'],\n",
    "    vdims=['parameter', 'ground_truth', 'mean', 'sd'],\n",
    ").opts(\n",
    "    color='parameter',\n",
    "    alpha=0.3,\n",
    "    xlim=(0, 1.05),\n",
    "    tools=['hover']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z-scores are great. There are a couple of points with terrible shrinkage that may be worth investigating. Generally $\\alpha$ shrinkage is great, $b$ less so. This may just reflect that my prior on $b$ was pretty tight; less than ~5% have shrinkage less than 0.9, and $\\gtrsim 90\\%$ have shrinkage above 0.98. I wonder too if the outliers are datasets that just had very very few mRNAs, which would make it hard to inform the posterior. Most of the $b$ points with poor shrinkage have $b >1$, which, if they also had very small $\\alpha$, is not inconsistent with my above guess. I should probably relax my priors on $\\alpha$ and especially $b$, now that I realized my mistake in plotting the prior predictives (forgot the `discrete` option).\n",
    "\n",
    "_Update_: after modifying my priors to be weaker, this looks perfectly fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the Posterior\n",
    "We already finished building the model in order to do SBC. Now we just run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do want posterior predictive checks this time\n",
    "data_UV5[\"ppc\"] = 1\n",
    "\n",
    "with bebi103.stan.disable_logging():\n",
    "    posterior_samples = sm.sample(data=data_UV5, cores=6)\n",
    "posterior_samples = az.from_cmdstanpy(\n",
    "    posterior_samples, posterior_predictive=[\"mRNA_counts_ppc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bebi103.stan.check_all_diagnostics(posterior_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, no sampler warnings. Let's visualize the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bokeh.io.show(\n",
    "    bebi103.viz.corner(\n",
    "        posterior_samples,\n",
    "        pars=[\"alpha\", \"b\"],\n",
    "        alpha=0.1,\n",
    "        xtick_label_orientation=np.pi / 4,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks quite reasonable. The transcripts per burst & burst frequency are both comparable to what we would have inferred from Manuel's MCMC, but now both parameters are actually identifiable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior predictive checks\n",
    "\n",
    "Even though the posterior looks ok, the model could still be wrong: it is identifiable, but is it consistent with the data? Posterior predictive checks address this by asking whether the model could plausibly generate the observed data. (Function borrowed from JB's finch beak tutorial for bebi103b 2020 TAs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppc_ecdfs(posterior_samples, df):\n",
    "    \"\"\"Plot posterior predictive ECDFs.\"\"\"\n",
    "    n_samples = (\n",
    "        posterior_samples.posterior_predictive.dims[\"chain\"]\n",
    "        * posterior_samples.posterior_predictive.dims[\"draw\"]\n",
    "    )\n",
    "\n",
    "    p1 = bebi103.viz.predictive_ecdf(\n",
    "        posterior_samples.posterior_predictive[\"mRNA_counts_ppc\"].values.reshape(\n",
    "            (n_samples, len(df))\n",
    "        ),\n",
    "        data=df[\"mRNA_cell\"],\n",
    "        discrete=True,\n",
    "        x_axis_label=\"mRNA counts per cells\",\n",
    "        frame_width=200,\n",
    "        frame_height=200\n",
    "    )\n",
    "\n",
    "    p2 = bebi103.viz.predictive_ecdf(\n",
    "        posterior_samples.posterior_predictive[\"mRNA_counts_ppc\"].values.reshape(\n",
    "            (n_samples, len(df))\n",
    "        ),\n",
    "        data=df[\"mRNA_cell\"],\n",
    "        percentiles=[95, 90, 80, 50],\n",
    "        diff=True,\n",
    "        discrete=True,\n",
    "        x_axis_label=\"mRNA counts per cells\",\n",
    "        frame_width=200,\n",
    "        frame_height=200\n",
    "    )\n",
    "    p1.x_range = p2.x_range\n",
    "    \n",
    "    return [p1, p2]\n",
    "\n",
    "bokeh.io.show(bokeh.layouts.gridplot(ppc_ecdfs(posterior_samples, df_UV5), ncols=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is not obviously wrong, but depending what percentiles I choose to plot, I might say that a few too many of the observed datapoints are outside the posterior predictive bands.\n",
    "\n",
    "Still, we should be systematic and plot the posterior and ppc for all the promoters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling all the data\n",
    "Let's repeat for all the constitutive promoters! Since we have so many, do separate loops to generate the samples and generate viz (so we can tweak viz without pointlessly rerunning the sampling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To supress output, check out JB's disable_logging wrapper in bebi103.stan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = {}\n",
    "for gene in df_unreg['experiment'].unique():\n",
    "    temp_df = df_unreg[df_unreg['experiment'] == gene]\n",
    "    data = copy.deepcopy(data_prior_pred)\n",
    "    data[\"N\"] = len(temp_df)\n",
    "    data[\"mRNA_counts\"] = temp_df[\"mRNA_cell\"].values.astype(int)\n",
    "    data[\"ppc\"] = 1\n",
    "    \n",
    "    with bebi103.stan.disable_logging():\n",
    "        posterior_samples = sm.sample(data=data, cores=6)\n",
    "    all_samples[gene] = az.from_cmdstanpy(\n",
    "        posterior_samples, posterior_predictive=[\"mRNA_counts_ppc\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always wise to check diagnostics (though for saving, I might supress the output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gene in all_samples:\n",
    "#     print(gene)\n",
    "#     bebi103.stan.check_all_diagnostics(all_samples[gene])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some run-to-run variation. Sometimes one or two promoters give Rhat warnings a bit over 1.01, but generally this looks ok.\n",
    "\n",
    "Now that we've sampled we can plot. (I can't figure out how to add titles to the bokeh layouts that `viz.corner` returns, so I'm doing the poor man's version for now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gene in all_samples:\n",
    "    print(gene)\n",
    "    # plot posterior as corner plot\n",
    "    bokeh.io.show(\n",
    "        bebi103.viz.corner(\n",
    "            all_samples[gene],\n",
    "            pars=[\"alpha\", \"b\"],\n",
    "            alpha=0.1,\n",
    "            xtick_label_orientation=np.pi / 4,\n",
    "        )\n",
    "    )\n",
    "    # plot post pred ecdf\n",
    "    obs_data = df_unreg[df_unreg[\"experiment\"] == gene]\n",
    "    bokeh.io.show(\n",
    "        bokeh.layouts.gridplot(ppc_ecdfs(all_samples[gene], obs_data), ncols=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive checks make it pretty clear that this most simple of models is not entirely capable of producing the observed data for all the promoters. The observed data has a pretty stereotyped pattern on the difference-in-ECDF plots. This strongly suggests there is something systematic our model fails to capture. Nevertheless, looking at the full ecdfs, I'm actually pleasantly surprised that the model does as well as it does. We haven't explicitly put in anything like cell cycle & gene copy number variability, and yet it captures quite a lot. Maybe it's already robust enough to attempt inference on simple repression data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation with predicted promoter binding energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to plot all 18 promoters together to compare their inferred burst parameters at a glance. Plotting all the posterior samples would be visually illegible.\n",
    "Instead we can compute a contour that encloses 95% of the samples. (Default smoothing in the contour calculator occasionally breaks and totally misses the HPD, so I increased it slightly.) Further, we can color code the contours by the predicted binding energies from Brewster/Jones 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First compute all the contours coords with JB's utility. (Note that `hv.Contours` prefers its input as a dictionary: `\"x\"` and `\"y\"` keys must be labeled as such and provide the coords of the contour, and a 3rd key providing a scalar for the contour level, here the promoter binding energy which we lookup.) Then we can overlay all the contours, colored by their corresponding promoter binding energy (fat lines make it easier to perceive the color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_list = []\n",
    "for gene in all_samples:\n",
    "    alpha_samples = all_samples[gene].posterior.alpha.values.flatten()\n",
    "    b_samples = all_samples[gene].posterior.b.values.flatten()\n",
    "    x_contour, y_contour = bebi103.viz.contour_lines_from_samples(\n",
    "        alpha_samples, b_samples, levels=0.95, smooth=0.025\n",
    "    )\n",
    "    contour_list.append(\n",
    "        {\n",
    "            \"x\": x_contour[0],\n",
    "            \"y\": y_contour[0],\n",
    "            \"Energy (kT)\": df_energies.loc[\n",
    "                df_energies[\"Name\"] == gene, \"Energy (kT)\"\n",
    "            ].values[0],\n",
    "            \"Promoter\": gene\n",
    "        }\n",
    "    )\n",
    "p = (\n",
    "    hv.Contours(contour_list, vdims=[\"Energy (kT)\", \"Promoter\"])\n",
    "    .opts(logx=True, logy=True)\n",
    "    .opts(line_width=2)\n",
    ")\n",
    "p.opts(\n",
    "    hv.opts.Contours(\n",
    "        cmap=\"viridis\",\n",
    "        colorbar=True,\n",
    "        tools=[\"hover\"],\n",
    "        width=500,\n",
    "        height=500,\n",
    "        xlabel=\"Î± (bursts per mRNA lifetime)\",\n",
    "        ylabel=\"b (transcripts per burst)\",\n",
    "        padding=0.03,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very interesting. By eye I'd say there's little to no correlation between burst size $b$ and binding energy, and maybe linear scaling between the log of burst frequency $\\alpha$ and binding energy, though that correlation is quite noisy. Can we intuit that relation theoretically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt at theory understanding\n",
    "Manuel observed how to relate burst parameters to thermodynamic model: just equate $\\langle m\\rangle$ in the two pictures. In the 2-state constitutive picture, we would have\n",
    "\\begin{align}\n",
    "\\langle m\\rangle = \\frac{r}{\\gamma}\\frac{k_{on}}{k_{on} + k_{off}}\n",
    "    \\approx \\frac{r}{\\gamma}\\frac{k_{on}}{k_{off}}\n",
    "\\end{align}\n",
    "in the limit $k_{off} \\gg k_{on}$ suggested by Manuel's inference. Taking the limit to the bursty one-state picture amounts to taking $r, k_{off}\\rightarrow\\infty$ while holding $r/k_{off}$ constant, and this ratio becomes the mean burst size $b$. In other words then\n",
    "\\begin{align}\n",
    "\\langle m\\rangle \\approx \\frac{r}{\\gamma}\\frac{k_{on}}{k_{off}}\n",
    "    \\approx b\\frac{k_{burst}}{\\gamma},\n",
    "\\end{align}\n",
    "where we have reinterpreted $k_{on}$ as the burst frequency $k_{burst}$ in the new picture.\n",
    "\n",
    "In the states-and-weights picture, we would have\n",
    "\\begin{align}\n",
    "\\langle m\\rangle =\n",
    "    \\frac{r}{\\gamma}\\frac{\\frac{P}{N_{NS}}e^{-\\beta\\Delta\\epsilon}}\n",
    "            {1+\\frac{P}{N_{NS}}e^{-\\beta\\Delta\\epsilon}}\n",
    "    \\approx \\frac{r}{\\gamma}\\frac{P}{N_{NS}}e^{-\\beta\\Delta\\epsilon}\n",
    "\\end{align}\n",
    "taking the usual weak promoter limit.\n",
    "\n",
    "Now we equate these two expressions for $\\langle m\\rangle$, i.e.,\n",
    "\\begin{align}\n",
    "\\langle m\\rangle \\approx b\\frac{k_{burst}}{\\gamma}\n",
    "    \\approx \\frac{r}{\\gamma}\\frac{P}{N_{NS}}e^{-\\beta\\Delta\\epsilon}.\n",
    "\\end{align}\n",
    "One could imagine several plausible associations between the various parameters. A post-facto argument for the \"right\" choice is the following.\n",
    "\n",
    "In the parametrization of negative binomial we are using, the Fano factor has the especially simple form $1+b$ (independent of $k_{burst}$). It would therefore be quite strange if $b\\ll1$ or $b\\gg10$, the former meaning the data is Poisson distributed to an absurd precision and the latter meaning it is absurdly overdisperse relative to Poisson. But $P/N_{NS}\\sim10^{-3}$, $r/\\gamma\\gg1$, and for various promoters we have anywhere from $e^{-\\beta\\Delta\\epsilon}\\lesssim10$ to $e^{-\\beta\\Delta\\epsilon}\\gtrsim10^3$. So then the only comibination that is anywhere near unity is $r/\\gamma \\times P/N_{NS}$ and thus the only associations that make any sense are\n",
    "\\begin{align}\n",
    "b &\\sim \\frac{r}{\\gamma}\\frac{P}{N_{NS}}\n",
    "\\\\\n",
    "\\frac{k_{burst}}{\\gamma} &\\sim e^{-\\beta\\Delta\\epsilon}.\n",
    "\\end{align}\n",
    "\n",
    "For a first check of this, set aside the variation in burst size $b$. Recalling we nondimensionalized $k_{burst}/\\gamma\\equiv\\alpha$, let us plot $\\log\\alpha$ vs the predicted $\\Delta G$ for each promoter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_alphas = np.zeros((len(df_energies), 3))\n",
    "for i, promoter in enumerate(df_energies.Name):\n",
    "    samples = all_samples[promoter].posterior.alpha.values.flatten()\n",
    "    log_alphas[i] = np.percentile(samples, (5, 50, 95))\n",
    "\n",
    "df_energies['log_alpha'] = np.log(log_alphas[:,1])\n",
    "df_energies['log_alpha_lbnd'] = df_energies['log_alpha'] - np.log(log_alphas[:,0])\n",
    "df_energies['log_alpha_ubnd'] = np.log(log_alphas[:,2]) - df_energies['log_alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holoviews ErrorBars wants a goofy format, can't take straight from dataframe\n",
    "err_bars = np.array(\n",
    "    (\n",
    "        df_energies[\"Energy (kT)\"],\n",
    "        df_energies[\"log_alpha\"],\n",
    "        df_energies[\"log_alpha_lbnd\"],\n",
    "        df_energies[\"log_alpha_ubnd\"],\n",
    "    )\n",
    ").transpose()\n",
    "\n",
    "(\n",
    "    hv.Points(\n",
    "        df_energies,\n",
    "        kdims=[\"Energy (kT)\", \"log_alpha\"],\n",
    "        vdims=[\"Name\", \"log_alpha_lbnd\", \"log_alpha_ubnd\"],\n",
    "    ).opts(tools=[\"hover\"])\n",
    "    * hv.ErrorBars(err_bars)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That resembles a straight line and it most certainly has the correct slope. The error bars are merely the uncertainty in our inference, and we shouldn't expect perfect agreement here since we've coarse-grained away so much detail. Realistically there should probably be horizontal error bars of at least 0.5 to 1 $kT$ on the predicted energies also."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
